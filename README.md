# Home_Sales
In this challenge, I use my knowledge of SparkSQL to determine key metrics about home sales data. I use Spark to create temporary views, partition the data, cache and uncache a temporary table, and verify that the table has been uncached.

Final Homework â€“ Data Bootcamp 2025
By Haley Armenta

ğŸ“Œ Overview
This project uses PySpark and SparkSQL to analyze a dataset of U.S. home sales. The assignment showcases core skills in distributed data processing, temporary views, caching, Parquet formatting, and performance benchmarking.

ğŸ› ï¸ Technologies Used
PySpark

SparkSQL

Google Colab / Jupyter Notebook

Pandas (lightly)

AWS S3 (for data sourcing)

ğŸ“‚ Data Source
home_sales_revised.csv

ğŸ” Analysis Breakdown
Task	Description
âœ… DataFrame Creation	Loaded CSV data from AWS S3
âœ… Temp View	Created home_sales temp table
âœ… Query 1	Avg price of 4-bedroom homes sold per year
âœ… Query 2	Avg price of 3 bed / 3 bath homes by year built
âœ… Query 3	Avg price of 3 bed / 3 bath homes with 2 floors & â‰¥ 2000 sqft
âœ… Query 4	Avg price by view rating (â‰¥ $350K), with runtime logging
âœ… Caching	Cached & verified home_sales table
âœ… Performance	Re-ran query using cache and measured runtime
âœ… Partitioning	Partitioned dataset by date_built into Parquet format
âœ… Parquet Temp View	Created parquet_home_sales table
âœ… Final Query	Reran query on Parquet data and measured runtime
âœ… Cleanup	Uncached temp table and verified memory release

ğŸ“ˆ Takeaways
Caching significantly improves query performance on repeated data access.

Partitioning data by relevant fields (like date_built) increases read efficiency.

SparkSQL is ğŸ”¥ for querying large datasets at scale.

ğŸ‰ Closing Notes
This was the final homework assignment of my bootcamp journey, and it was the most challenging â€” but also the most rewarding.

